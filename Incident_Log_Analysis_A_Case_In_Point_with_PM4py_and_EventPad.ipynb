{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Incident Log Analysis - A Case In Point with PM4py and EventPad",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yT4XkXoPt7O",
        "colab_type": "text"
      },
      "source": [
        "# Abstract\n",
        "\n",
        "In this paper we employ two different tools in order to gain insight into a support flow process log. With PM4py, a python library for process, we generate the core visual representations. With EventPad we apply further filtering and processing that allows certain events and traces to stand out. With these two tools we focus on process discovery, an essential pillar of process mining. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxV8drt6QIrn",
        "colab_type": "text"
      },
      "source": [
        "# 1. Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOfOjKJSs-SM",
        "colab_type": "text"
      },
      "source": [
        "As young as the process mining discipline is, it is increasinly applicable in our digital world. We can't imagine any modern enterprise without a variety of systems and often complex, or even complicated, application landscapes. In a potential digital jungle, everyone is challenged by the multitude of tasks. This fact could often lead to inefficiencies, on the one hand, and on the other to mis= or underexploited opportunities, depending on the business context. \n",
        "\n",
        "Many systems exist that aim at combatting these inefficiencies by orginizing and optimizing the workload operators have. Our spotlight here is on a  log produced by a prominent such system - ServiceNow. The log contains incident case data produced over the course of 11 months. This is the so-called \"pre-mortem\" data [7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLaXjQhTP3re",
        "colab_type": "text"
      },
      "source": [
        "# 2. Log Specification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHJ1ixgUs5jS",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Overview\n",
        "The log is freely available at [University of California Irvine's data set repository.](https://archive.ics.uci.edu/ml/datasets/Incident+management+process+enriched+event+log).[8] The data have been extracted from a ServiceNow instance and additionally enriched with data from supporting database. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ3AGMals3ar",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Volume and span\n",
        "The log contains a total of 141,707 events spread over *24,918* incidents and a total of 44 unique traces. Each incident is a case raised to the service desk. An event, or activity, can be one of the following:\n",
        "-\tNew\n",
        "- Active\n",
        "- Awaiting User Info\n",
        "- Awaiting Problem\n",
        "- Awaiting Vendor\n",
        "- Awaiting Evidence\n",
        "- Resolved\n",
        "- Closed\n",
        "\n",
        "We are in fact twisting the definition for \"event\"[***]. In this case we consider any status change, regardless from and to, as an event. In the support desk context, such a change indiciates certain actions on the case. \n",
        "\n",
        "The data spans a period of 11 months - from March to December 2016 and January 2017. To a large extent that makes it representative of a year's workload. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZHF8wHjs1RD",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Attributes \n",
        "The log contatins 36 total number of attributes: \n",
        "- 1 case identifier, \n",
        "- 1 state (activity) identifier and\n",
        "- 34 descriptive attributes. \n",
        "\n",
        "In this article, we are hardly interested in the descriptivce attributes. They will be explored and analyzed in a future work.\n",
        "\n",
        "Each event in the log refers to an activity and is related to a particular case as defined by [1]. An event log must also contain timestamp information for each activity. The log in point here complies with this definition, although with a remark. For the event attribute, we actually use the incident state attribute, as described in 2.2., which is always paired with the \"updated at\" attribute. In essence, this means that every  time the inident state changes, it is an event in itself. With the proposed process mining framework by [2] we aim at mining this log in order to identify key dependencies and statistics. For the full list of attributes, you can consult [3]. The relevant attributes for this analysis are presented here, as pairs consisting of the original attribute name and the naming from the definition:\n",
        "\n",
        "original attribute | definition naming\n",
        "--- | ---\n",
        "number | Case ID\n",
        "incident state | Event ID\n",
        "updated at | timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs8IN0QTsyIx",
        "colab_type": "text"
      },
      "source": [
        "## 2.4. Process Mining Type\n",
        "\n",
        "There are three types of process mining defined in [1]. These are discovery, conformance checking and enhancement. In this case we are going to focus on discovering patterns, as we do not have a predefined model to check and comply with as part of the conformance checking. Optimization is also out of the scope of this article, however, based on the discovery phase, inferences can be made. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czJr7AGfsrQL",
        "colab_type": "text"
      },
      "source": [
        "## 2.5. Importance\n",
        "The reason for choosing this log is the importance of the support process in software and IT development. Even the best and most complex products can be compromised greatly by inadequate, inefficient or untimely support effort. Therefore, insight into the support process could be crucial for any optimization effort.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZVM6hZSP3kk",
        "colab_type": "text"
      },
      "source": [
        "# 3. Tools and Methodology\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8FqUkHFtDqK",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Tools\n",
        "To perform the process mining in this case, we are going to employ the following two tools:\n",
        "- PM4py - the python lybrary for process mining. We will use it to mine the petri net of the incident log as well as for some basic statistics. [Here you can consult the project homepage.](http://pm4py.org/)\n",
        "- EventPad under academic license - the definitive tool for discovering patterns in event data by using visual analytics. [Homepage located here](http://www.event-pad.com/). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPV612GLtCJf",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Methodology\n",
        "As the incident log is not a classical event log, in the sense that there is no explicit end time attribute for each event,  we prefer employing IMDFb. It is a specific implementation of the Inductive Miner Directly Follows algorithm displayed by a Petri Net. IMDFb provides proper and enough heuristics in order to extract insightful information. \n",
        "\n",
        "(More deliberation what is IMDFb)\n",
        "\n",
        "Further to that, by using EventPad, we will make complete inventory of the log traces. We will then examine the 10 most common traces and the 10 most unique traces. (More importantly, we will try to correlate the working groups and priority to those traces.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRF7hOdAQW6w",
        "colab_type": "text"
      },
      "source": [
        "# 4. Mining the incident log with PM4py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIyVqO4tg7k",
        "colab_type": "text"
      },
      "source": [
        "Before diving into the mining process, let us take some time to delineate what a Petri Net is. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdQQ-g1btfmO",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. Petri Net formalization (notation)\n",
        "\n",
        "A Petri net is a triplet $N = (P,T,F)$, such that:\n",
        "- $P$ is a finite set of places, \n",
        "- $T$ is a finite set of transitions, such that\n",
        "  - $P \\cap T = \\emptyset$, and \n",
        "  - $F\\subseteq (P × T) \\cup (T ×P)$ is a set of directed arcs, called the flow relation.\n",
        "\n",
        "In reality, a Petri net could produce a representation that is not sound. Such a representation could have deadlock or livelocks. Therefore, a transition system such as a Petri net is sound \"if and only if from any reachable state it is possible to reach a state in Send.\" \n",
        "In the case of the incident log, the model is sound by nature because every case ends up with a status \"Closed\". If the incident log were streamed or a very short time period is being analyzed, then there is a chance to have non-sound model. \n",
        "\n",
        "We can argue that the Petri net is good for showing an overview of the process and event occurences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCFWU2tjtd0b",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 4.2. Petri net with frequency\n",
        "For the case in point this is the Petri net, as generated by PM4py:\n",
        "![pn](http://store.picbg.net/pubpic/EE/5D/dab55e4b6009ee5d.png)\n",
        "We have the following transitions $t_i$, each belonging to the set of transitions $T$:\n",
        "\n",
        "notation $t_i \\in T$ | event name | frequency\n",
        "--- | --- | ---\n",
        "$a$ | New | 36 407\n",
        "$b$ | Awaiting Problem | 461 \n",
        "$c$ | Awaiting Evidence | 38 \n",
        "$d$ | Awaiting Vendor | 707 \n",
        "$e$ | Active | 38 716\n",
        "$f$ | Awaiting User Info | 16 462\n",
        "$g$ | Closed | 24 985\n",
        "$h$ | Resolved | 25 751\n",
        "\n",
        "From the Petri net, some information about the incident becomes clear:\n",
        "- after an incident is open, it can proceed directly to either $b, c, d, e$ or $f$.\n",
        "- all 24 918 cases reach the end state $S^{end}$ (orange dot),thus proving the model is sound as there are no deadlocks or livelocks.\n",
        "- a total of 24 985 - 24918 = 67 cases had an  activity after they were closed\n",
        "- a total of 25 751 - 24 918 = 833 cases had an activity after they were resolved\n",
        "- almost all cases 24 908 had at least two changes on their activities, whereas only 10 cases had just one activity associated with them. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrtX_VrUtYim",
        "colab_type": "text"
      },
      "source": [
        "## 4.3. Heuristics net\n",
        "\n",
        "Now let us examine the same information but this time using the PM4py heuristics miner and the output heristics net:\n",
        "\n",
        "![hm](http://store.picbg.net/pubpic/05/A5/7738b7d837b005a5.png)\n",
        "\n",
        "Expectedly, we see the same numbers associated with the different activities.  The heuristics net, being very similar to a causal net, also displays how many times for each case, the actvity was changed to the same activity. This implies a change in the service desk ticket (user or case operator update) while the ticket does not necessarily progress to resolution.\n",
        "\n",
        "The following changes to the same activity are observed:\n",
        "\n",
        "notation $t_i \\in T$ | event name | frequency of change to itself\n",
        "--- | --- | ---\n",
        "$a$ | New | 20 010\n",
        "$b$ | Awaiting Problem | 208\n",
        "$c$ | Awaiting Evidence | 14 \n",
        "$d$ | Awaiting Vendor | 138\n",
        "$e$ | Active | 22 728\n",
        "$f$ | Awaiting User Info | 8790\n",
        "$g$ | Closed | 0\n",
        "$h$ | Resolved | 0\n",
        "\n",
        "From these statistics we can conclude:\n",
        "- that the most changes to the same status occur while the state is under the \"Active\" state.\n",
        "- almost all cases have a change of active to active, as 22 728 / 24 918 = 0.91 In the EventPad section of the article we will examine this visually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUTD_KTltTn1",
        "colab_type": "text"
      },
      "source": [
        "# 5. Timeseries statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jSOppYutKHA",
        "colab_type": "text"
      },
      "source": [
        "### 5.1. Reassignment count\n",
        "\n",
        "By leveraging pandas, we can easily model the event log into a timeseries. For the index value we set the 'opened_at' feature. Then we plot the reassignment count over time\n",
        "\n",
        "![alt text](http://store.picbg.net/pubpic/FD/88/fce8a4670a1afd88.png)\n",
        "\n",
        "From the plot we clearly observe a high concentration of reasignments in the months March - June. For the rest of the time frame, reassignments stay relatively low with some upsurge in the beginning of the nex year. \n",
        "\n",
        "Because we lack the exact context behind this data, it is hard to state how representative of the process this plot is. One the one hand, there could be cycle peaking between March - June. On the other hand, this could be a temporary spike."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPRbcqPctIhE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 5.2. Reopen count\n",
        "\n",
        "In exactly the same way, we generate a plot showing the reopen count of each case over time.\n",
        "\n",
        "![alt text](http://store.picbg.net/pubpic/A3/60/3559dab70624a360.png)\n",
        "\n",
        "We observe a slight positive correlation, although it is not the point to calculate it exactly here. Case reopening seems to peak at the same time frame as does the count of reassignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkX3-aLctHQ7",
        "colab_type": "text"
      },
      "source": [
        "### 5.3. average case duration\n",
        "\n",
        "To extract the maximum of our event log, we also implement some feature engineering. With the pandas API we calculate the case duration and save it as an extra dimension to the given log. We calcuate the case duraiton by subtracting the 'opened_at' and 'closed_at' values for each row. Then we filter the case durations by those that are greate than or equal to one, in order to ensure we have those ones that have laste over a day. In this case we are not interested in case duratios lasting less than that as they have been resolved within the day they were raised. \n",
        "\n",
        "```\n",
        "filtered_log = log[log.case_duration >= 1]\n",
        "ds_gr = filtered_log.groupby(['number']) \n",
        "\n",
        "filtered_log['case_duration'].mean()\n",
        "filtered_log['case_duration'].median()\n",
        "filtered_log['case_duration'].mode()\n",
        "\n",
        "filtered_log['case_duration'].min()\n",
        "filtered_log['case_duration'].max()\n",
        "```\n",
        "\n",
        "Further to that, we first group by the case number. This grouping makes our statistic relevent at the trace level - not at the event level for which we have no data. Secondly, we find that:\n",
        "- the median case duration is roughly 9 days;\n",
        "- the average case duration is roughly 17;\n",
        "- the mode is roughly 5\n",
        "- the minimum, naturally is 1 as the log is prefiltered, and\n",
        "- the maximum case duration is 341 days. \n",
        "\n",
        "Again, we observe that most of the cases that took the most time occured betweeb March - June:\n",
        "\n",
        "![alt text](http://store.picbg.net/pubpic/DC/BA/d141853d2f39dcba.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf7rU_D5yRR3",
        "colab_type": "text"
      },
      "source": [
        "# 5. Mining the incident log with EventPad\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIYsOqhztwPa",
        "colab_type": "text"
      },
      "source": [
        "## 5.1. Traces\n",
        "\n",
        "EventPad is a specialized software, aimed at process mining and deep insight into events. The reader can refer to [6] for more information about the product. \n",
        "\n",
        "After loading the source csv file into EventPad, we get a rundown of the 37 log traces in the traces section.\n",
        "In descending order of frequency, they are:\n",
        "\n",
        "nr | trace frequency | % of total change to itself | nr of activities \n",
        "--- | --- | --- | --- \n",
        "1 | 6421 | 25.77 | 3 \n",
        "2 | 4146 | 16.64 |4\n",
        "3 | 3015 | 12.10 |5\n",
        "4 | 2214 | 8.89 |6\n",
        "5 | 1813 | 7.28 |7\n",
        "6 | 1806 | 7.25 |2\n",
        "7 | 1332 | 5.35 |8\n",
        "8 | 1101 | 4.42 |9\n",
        "9 | 765 | 3.07 |10\n",
        "10 | 596 | 2.39 |11\n",
        "11 | 440 | 1.77 |12\n",
        "12 | 293| 1.18 |13\n",
        "13 | 222 | 0.89 |14\n",
        "14 | 173 | 0.69 |15\n",
        "15 | 122 | 0.49 |16\n",
        "16 | 106 | 0.43 |17\n",
        "17 | 74 | 0.30 |18\n",
        "18 | 66 | 0.26 |19\n",
        "19 | 41 | 0.16 |20\n",
        "20 | 33 | 0.13 |21\n",
        "21 | 30 | 0.12 |22\n",
        "22 | 18 | 0.07 |23\n",
        "23 | 15 | 0.06 |25\n",
        "24 | 12 | 0.05 |24\n",
        "25 | 10 | 0.04 |27\n",
        "26 | 9 | 0.04 |26\n",
        "27 | 8 | 0.03 |28\n",
        "28 | 6 | 0.02  |33\n",
        "29 | 4 | 0.02 |32\n",
        "30 | 4 | 0.02 |31\n",
        "31 | 4 | 0.02 |30\n",
        "32 | 4 | 0.02 |29\n",
        "33 | 3 | 0.01 |38\n",
        "34 | 2 | 0.01 |43\n",
        "35 | 1 | 0.001 |58\n",
        "36 | 1 | 0.001 |56\n",
        "37 | 1 | 0.001 |46\n",
        "38 | 1 | 0.001 |45\n",
        "39 | 1 | 0.001 |44\n",
        "40 | 1 | 0.001 |40\n",
        "41 | 1 | 0.001 |37\n",
        "42 | 1 | 0.001 | 36\n",
        "43 | 1 | 0.001 |35\n",
        "44 | 1 | 0.001 |34\n",
        "**total:** | **24 918** | **100** | - |\n",
        "\n",
        "The overall amount of unique patterns is 2077, i.e. these are completely unique traces occuring over this process lifecycle. \n",
        "\n",
        "The bottom 20 rows of the table indicate there are a 19 cases with equal frequency equal to or less than 10 (so pretty unique) that have a substantial amount of activities. For example, at row 34, we see that two traces span over 43 activities each. \n",
        "\n",
        "As suggested in [5], this log is a raltively \"small setting\" with its 24 918 traces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9pLzj_EtqLV",
        "colab_type": "text"
      },
      "source": [
        "## 5.2. Conditions \n",
        "\n",
        "One of the many powers of EventPad is the ability to apply rules to the event log by creating conditions based on the log attribute values. After the rule is applied, it can be associated with a specific \"glyph\", thus producing a powerful visual representation of the events. The colorful glyph can be clustered further as shown in 5.3. \n",
        "\n",
        "For visualizing the service desk activities, we created a couple of simple rules, one for each of the possible activities. In this wyay we assing a color for each activity state (New, Awaiting Problem, Awaiting Evidence, Awaiting Vendor, Active, Awaiting User Info, Closed, Resolved).\n",
        "\n",
        "Additionally, we sorted the traces by their absolute occurence in the log. As a result we have:\n",
        "- the 10 most common traces\n",
        "![most common](http://store.picbg.net/pubpic/BE/70/3bb33bff3bddbe70.PNG)\n",
        "We see that there is a total of 3238 + 3099 = 6337 traces with three activities. This number does not equal 6421, which is the total amount of traces with 3 activities. There appears to be 84 traces with three activities that appear less frequently. \n",
        "- the 10 most unique traces\n",
        "![most unique](http://store.picbg.net/pubpic/F6/CA/34f0a71953fbf6ca.PNG)\n",
        "Here if we take as an example the first trace with the 6 activities we see that it is only one of 2214 traces that have 6 activities. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP3sxZ4utn-0",
        "colab_type": "text"
      },
      "source": [
        "## 5.3. Activity Clustering \n",
        "\n",
        "To analyze further, we can construct the following condition in EventPad: highlight all 'Active' activities, where a change to Active is preceded by Active as well. \n",
        "\n",
        "Each colored glyph on the graph now corresponds to a change from Active to Active. We can now easily see cases that have seen many updates without a real change to their status.\n",
        "\n",
        "![a2a](http://store.picbg.net/pubpic/5E/A9/7dfdd762c1675ea9.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNBlg_BWs2kB",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion & future work\n",
        "\n",
        "This research is a simple case in point with our preferred tools: PM4py and EvetnPad. On the other hand, support processes are an integral part of every good software system. Analys based thereof could further optimize the activities. \n",
        "\n",
        "In the future we aim at not only extracting insight but also using that and the underlying data to construct a predictive model that estimates the duration of a support case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSVF7PkJPtmA",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "1. van der Aalst, W. 2012. Process mining: Overview and opportunities. ACM Trans. Manage. Inf. Syst. 3, 2,\n",
        "Article 7 (July 2012), 17 pages.\n",
        "DOI = 10.1145/2229156.2229157 http://doi.acm.org/10.1145/2229156.2229157\n",
        "2. Wil van der Aalst, Process Mining\n",
        "3. https://archive.ics.uci.edu/ml/datasets/Incident+management+process+enriched+event+log\n",
        "4. https://www.biorxiv.org/content/10.1101/047043v2\n",
        "5. Scalable Process Discovery with Guarantees\n",
        "Sander J.J. Leemans, Dirk Fahland, and Wil M.P. van der Aalst\n",
        "Eindhoven University of Technology, the Netherlands\n",
        "6. EventPad, home page: http://www.event-pad.com/\n",
        "7. Audit of e-Commerce Process, Manuela Aparicio\n",
        "ISCTE – University Institute of Lisbon , Adetti - IULLisboa, Portugal, José Leopoldo Nhampossa Universidade Eduardo MondlaneMaputo, Mozambique \n",
        "8. https://archive.ics.uci.edu/ml/datasets/Incident+management+process+enriched+event+log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkrQEjLAP-Hc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "d95aab9e-f4ad-43b9-a503-7a726ae1185a"
      },
      "source": [
        "pip install nbconvert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (5.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.6.1)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.4.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.1.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.4.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.10.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (1.12.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert) (4.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}